{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.cross_validation import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加上词向量特征\n",
    "w2v_feature = pd.read_csv('./data/w2v_feature.csv', header=0, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加上freq_feature\n",
    "# freq_feature = pd.read_csv('./data/freq_feature.csv', header=0, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.read_csv('./data/all_data', sep=' ', header=None, encoding='utf-8')\n",
    "all_data.columns = ['id', 'sent1', 'sent2', 'label']\n",
    "feature_data = pd.read_csv('./data/feature_table.csv', header=0, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_data = pd.concat([feature_data, w2v_feature], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_data.drop(columns=['q1_hash', 'q2_hash'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>shared_word</th>\n",
       "      <th>tfidf_shared</th>\n",
       "      <th>tfidf_dif</th>\n",
       "      <th>word_len1</th>\n",
       "      <th>word_len2</th>\n",
       "      <th>char_len1</th>\n",
       "      <th>char_len2</th>\n",
       "      <th>length_dif</th>\n",
       "      <th>length_dif_rate</th>\n",
       "      <th>common_words</th>\n",
       "      <th>...</th>\n",
       "      <th>dup_sent_3</th>\n",
       "      <th>dup_sent_4</th>\n",
       "      <th>ngram_jac_1</th>\n",
       "      <th>ngram_jac_2</th>\n",
       "      <th>ngram_jac_3</th>\n",
       "      <th>ngram_di_1</th>\n",
       "      <th>ngram_di_2</th>\n",
       "      <th>ngram_di_3</th>\n",
       "      <th>w2v_cos</th>\n",
       "      <th>w2v_idf_cos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.577535</td>\n",
       "      <td>11.627644</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.817394</td>\n",
       "      <td>0.810542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.580645</td>\n",
       "      <td>0.669126</td>\n",
       "      <td>7.722772</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.409091</td>\n",
       "      <td>0.260870</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.580645</td>\n",
       "      <td>0.413793</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.879860</td>\n",
       "      <td>0.918389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.774194</td>\n",
       "      <td>0.669747</td>\n",
       "      <td>24.605322</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.482759</td>\n",
       "      <td>0.296296</td>\n",
       "      <td>0.921079</td>\n",
       "      <td>0.909509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.620690</td>\n",
       "      <td>0.539571</td>\n",
       "      <td>0.362998</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.518519</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.876567</td>\n",
       "      <td>0.839398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.307486</td>\n",
       "      <td>8.166710</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.476190</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.896401</td>\n",
       "      <td>0.748673</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 85 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   shared_word  tfidf_shared  tfidf_dif  word_len1  word_len2  char_len1  \\\n",
       "0     0.545455      0.577535  11.627644          4          4          7   \n",
       "1     0.580645      0.669126   7.722772          5          5         11   \n",
       "2     0.774194      0.669747  24.605322          6          7         10   \n",
       "3     0.620690      0.539571   0.362998          7          6         11   \n",
       "4     0.521739      0.307486   8.166710          5          5          8   \n",
       "\n",
       "   char_len2  length_dif  length_dif_rate  common_words     ...       \\\n",
       "0          8           1         0.875000             4     ...        \n",
       "1         10           1         0.909091             6     ...        \n",
       "2         13           3         0.769231             9     ...        \n",
       "3         10           1         0.909091             6     ...        \n",
       "4         10           2         0.800000             5     ...        \n",
       "\n",
       "   dup_sent_3  dup_sent_4  ngram_jac_1  ngram_jac_2  ngram_jac_3  ngram_di_1  \\\n",
       "0           9           1     0.375000     0.250000     0.125000    0.545455   \n",
       "1           1           1     0.409091     0.260870     0.125000    0.580645   \n",
       "2           1           1     0.666667     0.318182     0.173913    0.800000   \n",
       "3           4           1     0.473684     0.350000     0.250000    0.642857   \n",
       "4           8           1     0.352941     0.312500     0.266667    0.521739   \n",
       "\n",
       "   ngram_di_2  ngram_di_3   w2v_cos  w2v_idf_cos  \n",
       "0    0.400000    0.222222  0.817394     0.810542  \n",
       "1    0.413793    0.222222  0.879860     0.918389  \n",
       "2    0.482759    0.296296  0.921079     0.909509  \n",
       "3    0.518519    0.400000  0.876567     0.839398  \n",
       "4    0.476190    0.421053  0.896401     0.748673  \n",
       "\n",
       "[5 rows x 85 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_data.columns.values.tolist()\n",
    "feature_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_x = feature_data.values\n",
    "all_y = all_data.label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection based on ANOVA-f score 特征选择(降维)\n",
    "# selectK = SelectKBest(f_classif, k=30)\n",
    "# selectK.fit(all_x, all_y)\n",
    "# all_x = selectK.transform(all_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = all_x[:10000]\n",
    "y_test = all_y[:10000]\n",
    "x = all_x[10000:]\n",
    "y = all_y[10000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_train = xgb.DMatrix(data=x, label=y)\n",
    "xgb_test = xgb.DMatrix(data=x_test, label=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F1值度量方法\n",
    "def threshold(i):\n",
    "    if i > 0.20:\n",
    "        return 1.0\n",
    "    else:\n",
    "        return 0.0\n",
    "def f1_metric(y_pred, train_data):\n",
    "    y_true = train_data.get_label()\n",
    "    #y_pred = np.round(y_pred)\n",
    "    y_pred = list(map(threshold, y_pred))\n",
    "    return 'F1', f1_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters = {\n",
    "#             'booster':'gbtree',\n",
    "#             'objective':'binary:logistic',\n",
    "#             'eta': list(np.linspace(1, 9, 17)),\n",
    "#             'max_depth':list(range(3, 10, 1)),\n",
    "#             'subsample':list(np.linspace(0.5, 1, 6)),\n",
    "#             'min_child_weight': list(range(1,10,1)),\n",
    "#             'colsample_bytree':list(np.linspace(0.5, 1, 6)),\n",
    "#             'scale_pos_weight':list(np.linspace(0, 0.5, 6)),\n",
    "#             'eval_metric':'logloss',\n",
    "#             'gamma':list(np.linspace(0, 1, 11))          \n",
    "# }\n",
    "parameters = {\n",
    "            'booster':'gbtree',\n",
    "            'objective':'binary:logistic',\n",
    "            'eta':0.2,\n",
    "            'max_depth':10,\n",
    "            'subsample':1.0,\n",
    "            'min_child_weight':2,\n",
    "            'colsample_bytree':0.8,\n",
    "            'scale_pos_weight':0.5,\n",
    "            'eval_metric':'logloss',\n",
    "            'gamma':0.2,            \n",
    "            'lambda':0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "watchlist = [(xgb_train,'train'),(xgb_test,'val')]\n",
    "xgb_model = xgb.train(params=parameters,\n",
    "                      dtrain=xgb_train,\n",
    "                      num_boost_round=5000,\n",
    "                      evals=watchlist,\n",
    "                      early_stopping_rounds=100,\n",
    "                      feval=f1_metric,\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model = pd.read_csv('./training_data/cnn_model.csv', header=0)  # cnn预测结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_model = pd.read_csv('./training_data/rnn_model.csv', header=0)  # rnn预测结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntrain = len(x)\n",
    "ntest = len(x_test)\n",
    "SEED = 0 # for reproducibility\n",
    "NFOLDS = 5 # set folds for out-of-fold prediction\n",
    "k_fold = KFold(len(x), n_folds=NFOLDS, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SklearnHelper(object):\n",
    "    def __init__(self, clf, seed=0, params=None):\n",
    "        params['random_state'] = seed\n",
    "        self.clf = clf(**params)\n",
    "\n",
    "    def train(self, x_train, y_train):\n",
    "        self.clf.fit(x_train, y_train)\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.clf.predict(x)\n",
    "    \n",
    "    def fit(self,x,y):\n",
    "        return self.clf.fit(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_oof(clf, x_train, y_train, x_test):\n",
    "    oof_train = np.zeros((ntrain,))\n",
    "    oof_test = np.zeros((ntest,))\n",
    "    oof_test_skf = np.empty((NFOLDS, ntest))\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(k_fold):\n",
    "        x_tr = x_train[train_index]\n",
    "        y_tr = y_train[train_index]\n",
    "        x_te = x_train[test_index]\n",
    "\n",
    "        clf.train(x_tr, y_tr)\n",
    "\n",
    "        oof_train[test_index] = clf.predict(x_te)\n",
    "        oof_test_skf[i, :] = clf.predict(x_test)\n",
    "\n",
    "    oof_test[:] = oof_test_skf.mean(axis=0)\n",
    "    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_params = {\n",
    "    'n_jobs': -1,\n",
    "    'n_estimators': 500,\n",
    "     'warm_start': True, \n",
    "     #'max_features': 0.2,\n",
    "    'max_depth': 6,\n",
    "    'min_samples_leaf': 2,\n",
    "    'max_features' : 'sqrt',\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "# Extra Trees Parameters\n",
    "et_params = {\n",
    "    'n_jobs': -1,\n",
    "    'n_estimators':500,\n",
    "    #'max_features': 0.5,\n",
    "    'max_depth': 8,\n",
    "    'min_samples_leaf': 2,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "# AdaBoost parameters\n",
    "ada_params = {\n",
    "    'n_estimators': 500,\n",
    "    'learning_rate' : 0.75\n",
    "}\n",
    "\n",
    "# Gradient Boosting parameters\n",
    "gb_params = {\n",
    "    'n_estimators': 500,\n",
    "     #'max_features': 0.2,\n",
    "    'max_depth': 5,\n",
    "    'min_samples_leaf': 2,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "# Support Vector Classifier parameters \n",
    "svc_params = {\n",
    "    'kernel' : 'linear',\n",
    "    'C' : 0.025\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = SklearnHelper(clf=RandomForestClassifier, seed=SEED, params=rf_params)\n",
    "et = SklearnHelper(clf=ExtraTreesClassifier, seed=SEED, params=et_params)\n",
    "ada = SklearnHelper(clf=AdaBoostClassifier, seed=SEED, params=ada_params)\n",
    "gb = SklearnHelper(clf=GradientBoostingClassifier, seed=SEED, params=gb_params)\n",
    "svc = SklearnHelper(clf=SVC, seed=SEED, params=svc_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y\n",
    "x_train = x\n",
    "x_test = x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "et_oof_train, et_oof_test = get_oof(et, x_train, y_train, x_test) # Extra Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/ensemble/forest.py:305: UserWarning: Warm-start fitting without increasing n_estimators does not fit new trees.\n",
      "  warn(\"Warm-start fitting without increasing n_estimators does not \"\n"
     ]
    }
   ],
   "source": [
    "rf_oof_train, rf_oof_test = get_oof(rf,x_train, y_train, x_test) # Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_oof_train, ada_oof_test = get_oof(ada, x_train, y_train, x_test) # AdaBoost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_oof_train, gb_oof_test = get_oof(gb,x_train, y_train, x_test) # Gradient Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_oof_train, svc_oof_test = get_oof(svc,x_train, y_train, x_test) # Support Vector Classifier"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
