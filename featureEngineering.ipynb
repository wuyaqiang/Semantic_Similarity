{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import re\n",
    "import math\n",
    "import jieba\n",
    "import json\n",
    "import codecs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from collections import Counter\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import f1_score\n",
    "jieba.load_userdict('./dict/jieba_dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1 = pd.read_csv('./data/atec_nlp_sim_train_add.csv', sep='\\t', header=None, encoding='utf-8')\n",
    "data_1.columns = ['id', 'question1', 'question2', 'is_duplicate']\n",
    "data_2 = pd.read_csv('./data/atec_nlp_sim_train.csv', sep='\\t', header=None, encoding='utf-8')\n",
    "data_2.columns = ['id', 'question1', 'question2', 'is_duplicate']\n",
    "all_data = pd.concat([data_1, data_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>我要取消预约理财</td>\n",
       "      <td>怎么取消我的定期理财</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>我银行卡密码正确，但是错误的。</td>\n",
       "      <td>密码输入正确，提示错误</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>自动还款是扣余额宝的吗</td>\n",
       "      <td>余额宝有钱，还款当日自动扣钱吗</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>赔付的运费险什么时候到账</td>\n",
       "      <td>我退货了，什么时候退运费给我？</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>余额宝的***转不出来了吗</td>\n",
       "      <td>为什么我的余额宝转不进去？</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id        question1        question2  is_duplicate\n",
       "0   1         我要取消预约理财       怎么取消我的定期理财             0\n",
       "1   2  我银行卡密码正确，但是错误的。      密码输入正确，提示错误             1\n",
       "2   3      自动还款是扣余额宝的吗  余额宝有钱，还款当日自动扣钱吗             1\n",
       "3   4     赔付的运费险什么时候到账  我退货了，什么时候退运费给我？             0\n",
       "4   5    余额宝的***转不出来了吗    为什么我的余额宝转不进去？             0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data = pd.read_csv('./data/all_data', sep=' ', header=None, encoding='utf-8')\n",
    "all_data.columns = ['id', 'question1', 'question2', 'is_duplicate']\n",
    "all_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jieba_cut(sentence):\n",
    "    '''jieba分词'''\n",
    "    seg_sent = jieba.cut(sentence, cut_all=False)\n",
    "    return list(seg_sent)\n",
    "\n",
    "def jieba_word_cut(sentence):\n",
    "    '''切成字，并且保留切的词'''\n",
    "    sentence_list = []\n",
    "    for word in sentence:\n",
    "        if len(word) == 1:\n",
    "            sentence_list.append(word)\n",
    "        else:\n",
    "            for character in word:\n",
    "                sentence_list.append(character)\n",
    "            sentence_list.append(word)\n",
    "    return ' '.join(sentence_list)\n",
    "\n",
    "def first_char_cut(sentence):\n",
    "    '''jieba分词'''\n",
    "    first_char = []\n",
    "    for word in sentence:\n",
    "        if len(word) == 1:\n",
    "            first_char.append(word)\n",
    "        else:\n",
    "            first_char.append(word[0])\n",
    "    return ' '.join(first_char)\n",
    "\n",
    "def word_cut(sentence):\n",
    "    '''分字'''\n",
    "    return ' '.join([word for word in sentence])\n",
    "\n",
    "def replace_words(sentence):\n",
    "    #stopwords = ['\"', '#', '&', \"'\", '(', ')', '*', '+',',', '-', '.', '...', '/', ':', ';', '<', '=','>', '?', '@', 'Lex', '[', ']', 'exp', 'sub', 'sup', '}', '~', '·', '×', '÷', 'Δ', 'Ψ', 'γ', 'μ', 'φ', 'В', '—', '———', '‘', '’', '“', '”', '″', '℃', 'Ⅲ', '↑', '→', '∈', '①','②', '③', '④', '⑤', '⑥', '⑦', '⑧', '⑨', '⑩', '──', '■', '▲', '、', '。', '〉', '《', '》', '『', '』', '【', '】', '〔', '〕', '㈧', '一','丫', '也', '了', '俺', '俺们', '兮', '吧', '吧哒', '呃', '呐', '呗', '咚', '咦', '咧', '哎', '哎呀','哎哟','哩','唉', '啊', '啐','喏', '喔', '喔唷', '嗬', '嗯', '嗳', '我', '把', '按', '按照', '数', '日', '的', '罢了', '蚁', '蚂', '蜂', '逼', '阿', '！', '＃', '％', '＆', '＇', '（', '）', '＊', '＋', '，', '－', '．', '／', '１', '：', '；', '＜', '＝','＞', '＞λ', '？', 'Ａ', 'Ｂ', 'Ｆ', 'Ｉ', 'Ｌ', 'ＬＩ', 'Ｔ', 'Ｘ', 'Ｚ', '［', '［－', '］', '＿', 'ａ', '｛', '｝', '～', '～±','～＋']\n",
    "    replace_dict = {'零':'0','一':'1','二':'2','三':'3','四':'4','五':'5','六':'6','七':'7','八':'8','九':'9','十':'10','叻': '了', '童': '通', '职': '值', '電': '电', '刪': '删', '宫': '营', '轲': '可', '兩': '两', '泽': '择', '拱': '供', '貝': '呗', '夠': '够', '罰': '罚', '嚒': '么', '涉': '设', '愈': '逾', '唄': '呗', '為': '为', '現': '现', '珊': '删', '酱': '降', '讷': '呐', '杞': '从', '竖': '公', '戗': '钱', '凊': '清', '陪': '赔', '嫌': '限', '鹅': '额', '聪': '充', '個': '个', '亿': '已', '蚂议': '蚂蚁', '陶': '淘', '伏': '付', '堡': '宝', '肔': '服', '巳': '已', '花坝': '花呗', '洼': '清', '甜': '填', '厉': '里', '渣': '咋', '纬': '为', '無': '无', '勇': '用', '扭': '钮', '压金': '押金', '绐': '给', '捉': '提', '喻': '逾', '換': '还', '胞': '宝', '調': '调', '抄': '超', '麽': '么', '歆': '完', '囗': '口', '卜': '不', '扮': '办', '氣': '气', '費': '费', '評': '评', '夫': '付', '猛': '能', '銀': '银', '枪': '清', '痛': '通', '喀': '额', '囙': '回', '筠': '运', '昰': '是', '吾': '我', '帳': '账', '玉': '与', '浓': '弄', '雪': '学', '螞蟻': '蚂蚁', '規': '规', '丕': '不', '時': '时', '花唄': '花呗', '梓': '么', '济': '机', '弍': '式', '貸': '贷', '繳': '交', '届': '借', '甪': '用', '蒋': '降', '欺': '期', '舍': '设', '妃': '为', '咔': '卡', '啤': '碑', '傲': '以', '俞': '逾', '田': '天', '剛': '刚', '怼': '对', '脚': '交', '怅': '账', '餮': '餐', '栓': '删', '揍': '款', '寶': '宝', '蝙': '变', '肖': '消', '剪': '减', '崔': '催', '榜': '绑', '扎': '咋', '圆': '元', '饯': '钱', '嘞': '了', '腐': '付', '辞': '迟', '昱': '里', '师': '是', '侍': '待', '睌': '晚', '宣': '删', '花背': '花呗', '購': '购', '慨': '概', '魔': '摩', '臂': '呗', '肿': '怎', '花贝': '花呗', '碼': '码', '茨': '款', '拳': '券', '乍': '咋', '証': '证', '歧': '期', '嘟': '都', '结呗': '借呗', '锤': '吹', '轻': '清', '厚': '后', '玏': '功', '乙': '已', '挷': '绑', '拦': '栏', '辟': '批', '讠': '之', '闹': '弄', '負': '付', '犹': '怀', '筘': '扣', '嗳': '爱', '說': '说', '扔': '仍', '花裁': '花', '吋': '时', '収': '收', '磕': '可', '給': '给', '腿': '退', '梆': '绑', '冬': '冻', '幼': '动', '炸': '咋', '經': '经', '骂': '吗', '欹': '款', '莉': '里', '叶': '页', '鍀': '的', '岀': '出', '欲': '逾', '花被': '花呗', '节清': '结清', '錢': '钱', '曰': '日', '戒备': '借呗', '灣': '湾', '贺': '和', '紅': '红', '幺': '么', '孒': '了', '別': '别', '涮': '刷', '歉': '欠', '泥': '呢', '額': '额', '栅': '删', '佝': '何', '壮': '状', '叹': '呗', '眷': '劵', '洋': '样', '蚂蚊': '蚂蚁', '哩': '里', '蚂蚱': '蚂蚁', '還': '还', '樣': '样', '杳': '查', '茌': '花', '卷': '券', '證': '证', '麼': '么', '佘': '余', '買': '买', '帝': '低', '胀': '账', '雨': '与', '花能': '花呗能', '崴': '为', '貨': '货', '丟': '丢', '開': '开', '叭': '呗', '昵': '呢', '祝': '况', '毙': '闭', '屎': '是', '佰': '百', '宴': '延', '幵': '开', '仟': '千', '來': '来', '挨': '爱', '祢': '你', '糸': '细', '颃': '用', '乳': '用', '借唄': '借呗', '唔': '客', '則': '则', '阔': '可', '叨': '嘛', '花多上': '花多少', '镀': '度', '刭': '到', '冯': '吗', '蔡': '才', '丽': '里', '減': '减', '狂': '款', '錯': '错', '匆': '充', '問': '问', '窃': '切', '贯': '关', '勒': '了', '颌': '额', '敗': '败', '咬': '要', '鈤': '日', '莪': '我', '腨': '用', '吵': '超', '篮': '蓝', '培': '赔', '粑': '把', '躲': '多', '嗎': '吗', '戶': '户', '毎': '每', '呮': '呗', '姑': '过', '胆': '但', '脱': '拖', '胃': '为', '剧': '刷', '吸': '息', '布': '不', '夂': '久', '栋': '冻', '淸': '清', '萌': '能', '愉': '逾', '請': '请', '卅': '啥', '堤': '提', '吱': '支', '禾': '何', '菅': '营', '渝': '逾', '侯': '候', '權': '权', '鼻': '比', '杜': '度', '嗨': '还', '踩': '才', '矿': '款', '珐': '法', 'ma': '吗', '花百': '花呗', '绊': '绑', '甬': '通', '車': '车', '叧': '另', '述': '诉', '査': '查', '瞪': '登', '機': '机', '啲': '的', '設': '设', '綁': '绑', '遲': '迟', '赞': '暂', '粤': '月', '驗': '验', '説': '说', '花花': '花', '坝': '呗', '发呗': '花呗', '虫': '了', '臨': '临', '笫': '第', '廷': '延', '琪': '期', '扥': '等', '谝': '骗', '倩': '欠', '挑': '调', '⑩': '', '/': '', '‘': '', '哩': '', '～±': '', '／': '', '｛': '', ';': '', '：': '', '％': '', '＝': '', '按': '', '喏': '', '>': '', '俺们': '', '》': '', 'Ⅲ': '', '蜂': '', '*': '', '日': '', '=': '', '⑦': '', '～': '', '）': '', ']': '', '。': '', ',': '', '“': '', '}': '', '逼': '', '＞': '', 'ＬＩ': '', '&': '', '(': '', \"'\": '', '哎哟': '', '数': '', 'sub': '', '！': '', '~': '', '@': '', '∈': '', '咦': '', '?': '', '喔唷': '', '⑤': '', '①': '', 'μ': '', '、': '', 'γ': '', '嗳': '', '』': '', '一': '', '②': '', 'Ｉ': '', 'В': '', '＃': '', '兮': '', '我': '', '［': '', '＋': '', '把': '', 'Ｌ': '', '<': '', '俺': '', '──': '', '⑧': '', '[': '', '④': '', 'sup': '', '哎呀': '', '；': '', '哎': '', 'Ｂ': '', '÷': '', '呗': '', '阿': '', '喔': '', '蚂': '', 'ａ': '', '#': '', 'Ｆ': '', '〔': '', ':': '', '吧': '', '丫': '', '嗯': '', '的': '', '■': '', '”': '', 'Ｔ': '', '＇': '', '《': '', '啐': '', '也': '', '嗬': '', '㈧': '', 'φ': '', '\"': '', '↑': '', 'Δ': '', 'Ψ': '', '℃': '', '⑥': '', '〕': '', '+': '', 'exp': '', '＆': '', '罢了': '', '·': '', '″': '', '—': '', '１': '', '〉': '', '...': '', '＊': '', 'Lex': '', '＿': '', '蚁': '', '啊': '', '『': '', '．': '', '【': '', '呃': '', '［－': '', '▲': '', '，': '', '’': '', '｝': '', '（': '', '－': '', '】': '', '×': '', '咧': '', '.': '', '了': '', ')': '', 'Ａ': '', 'Ｘ': '', '咚': '', '］': '', '？': '', '→': '', '③': '', '＜': '', '吧哒': '', '按照': '', '唉': '', '＞λ': '', '———': '', '-': '', 'Ｚ': '', '⑨': '', '～＋': '', '呐': ''}\n",
    "    for key, value in replace_dict.items():\n",
    "        sentence = sentence.replace(key, value)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "替换词:  78.54145288467407\n",
      "jieba分词： 42.33256936073303\n",
      "分字+词:  2.2613556385040283\n",
      "单词首字:  1.2675347328186035\n",
      "分字:  1.0780537128448486\n",
      "总时间： 125.48151326179504\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>jieba_q1</th>\n",
       "      <th>jieba_q2</th>\n",
       "      <th>jieba_word_cut_q1</th>\n",
       "      <th>jieba_word_cut_q2</th>\n",
       "      <th>first_char_q1</th>\n",
       "      <th>first_char_q2</th>\n",
       "      <th>word_cut_q1</th>\n",
       "      <th>word_cut_q2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>要取消预约理财</td>\n",
       "      <td>怎么取消定期理财</td>\n",
       "      <td>0</td>\n",
       "      <td>[要, 取消, 预约, 理财]</td>\n",
       "      <td>[怎么, 取消, 定期, 理财]</td>\n",
       "      <td>要 取 消 取消 预 约 预约 理 财 理财</td>\n",
       "      <td>怎 么 怎么 取 消 取消 定 期 定期 理 财 理财</td>\n",
       "      <td>要 取 预 理</td>\n",
       "      <td>怎 取 定 理</td>\n",
       "      <td>要 取 消 预 约 理 财</td>\n",
       "      <td>怎 么 取 消 定 期 理 财</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>银行卡密码正确但是错误</td>\n",
       "      <td>密码输入正确提示错误</td>\n",
       "      <td>1</td>\n",
       "      <td>[银行卡, 密码, 正确, 但是, 错误]</td>\n",
       "      <td>[密码, 输入, 正确, 提示, 错误]</td>\n",
       "      <td>银 行 卡 银行卡 密 码 密码 正 确 正确 但 是 但是 错 误 错误</td>\n",
       "      <td>密 码 密码 输 入 输入 正 确 正确 提 示 提示 错 误 错误</td>\n",
       "      <td>银 密 正 但 错</td>\n",
       "      <td>密 输 正 提 错</td>\n",
       "      <td>银 行 卡 密 码 正 确 但 是 错 误</td>\n",
       "      <td>密 码 输 入 正 确 提 示 错 误</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>自动还款是扣余额宝吗</td>\n",
       "      <td>余额宝有钱还款当自动扣钱吗</td>\n",
       "      <td>1</td>\n",
       "      <td>[自动, 还款, 是, 扣, 余额宝, 吗]</td>\n",
       "      <td>[余额宝, 有钱, 还款, 当, 自动, 扣钱, 吗]</td>\n",
       "      <td>自 动 自动 还 款 还款 是 扣 余 额 宝 余额宝 吗</td>\n",
       "      <td>余 额 宝 余额宝 有 钱 有钱 还 款 还款 当 自 动 自动 扣 钱 扣钱 吗</td>\n",
       "      <td>自 还 是 扣 余 吗</td>\n",
       "      <td>余 有 还 当 自 扣 吗</td>\n",
       "      <td>自 动 还 款 是 扣 余 额 宝 吗</td>\n",
       "      <td>余 额 宝 有 钱 还 款 当 自 动 扣 钱 吗</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>赔付运费险什么时候到账</td>\n",
       "      <td>退货什么时候退运费给</td>\n",
       "      <td>0</td>\n",
       "      <td>[赔付, 运费, 险, 什么, 时候, 到, 账]</td>\n",
       "      <td>[退货, 什么, 时候, 退, 运费, 给]</td>\n",
       "      <td>赔 付 赔付 运 费 运费 险 什 么 什么 时 候 时候 到 账</td>\n",
       "      <td>退 货 退货 什 么 什么 时 候 时候 退 运 费 运费 给</td>\n",
       "      <td>赔 运 险 什 时 到 账</td>\n",
       "      <td>退 什 时 退 运 给</td>\n",
       "      <td>赔 付 运 费 险 什 么 时 候 到 账</td>\n",
       "      <td>退 货 什 么 时 候 退 运 费 给</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>余额宝转不出来吗</td>\n",
       "      <td>为什么余额宝转不进去</td>\n",
       "      <td>0</td>\n",
       "      <td>[余额宝, 转, 不, 出来, 吗]</td>\n",
       "      <td>[为什么, 余额宝, 转, 不, 进去]</td>\n",
       "      <td>余 额 宝 余额宝 转 不 出 来 出来 吗</td>\n",
       "      <td>为 什 么 为什么 余 额 宝 余额宝 转 不 进 去 进去</td>\n",
       "      <td>余 转 不 出 吗</td>\n",
       "      <td>为 余 转 不 进</td>\n",
       "      <td>余 额 宝 转 不 出 来 吗</td>\n",
       "      <td>为 什 么 余 额 宝 转 不 进 去</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id    question1      question2  is_duplicate                   jieba_q1  \\\n",
       "0   1      要取消预约理财       怎么取消定期理财             0            [要, 取消, 预约, 理财]   \n",
       "1   2  银行卡密码正确但是错误     密码输入正确提示错误             1      [银行卡, 密码, 正确, 但是, 错误]   \n",
       "2   3   自动还款是扣余额宝吗  余额宝有钱还款当自动扣钱吗             1     [自动, 还款, 是, 扣, 余额宝, 吗]   \n",
       "3   4  赔付运费险什么时候到账     退货什么时候退运费给             0  [赔付, 运费, 险, 什么, 时候, 到, 账]   \n",
       "4   5     余额宝转不出来吗     为什么余额宝转不进去             0         [余额宝, 转, 不, 出来, 吗]   \n",
       "\n",
       "                      jieba_q2                      jieba_word_cut_q1  \\\n",
       "0             [怎么, 取消, 定期, 理财]                 要 取 消 取消 预 约 预约 理 财 理财   \n",
       "1         [密码, 输入, 正确, 提示, 错误]  银 行 卡 银行卡 密 码 密码 正 确 正确 但 是 但是 错 误 错误   \n",
       "2  [余额宝, 有钱, 还款, 当, 自动, 扣钱, 吗]          自 动 自动 还 款 还款 是 扣 余 额 宝 余额宝 吗   \n",
       "3       [退货, 什么, 时候, 退, 运费, 给]      赔 付 赔付 运 费 运费 险 什 么 什么 时 候 时候 到 账   \n",
       "4         [为什么, 余额宝, 转, 不, 进去]                 余 额 宝 余额宝 转 不 出 来 出来 吗   \n",
       "\n",
       "                           jieba_word_cut_q2  first_char_q1  first_char_q2  \\\n",
       "0                怎 么 怎么 取 消 取消 定 期 定期 理 财 理财        要 取 预 理        怎 取 定 理   \n",
       "1         密 码 密码 输 入 输入 正 确 正确 提 示 提示 错 误 错误      银 密 正 但 错      密 输 正 提 错   \n",
       "2  余 额 宝 余额宝 有 钱 有钱 还 款 还款 当 自 动 自动 扣 钱 扣钱 吗    自 还 是 扣 余 吗  余 有 还 当 自 扣 吗   \n",
       "3            退 货 退货 什 么 什么 时 候 时候 退 运 费 运费 给  赔 运 险 什 时 到 账    退 什 时 退 运 给   \n",
       "4             为 什 么 为什么 余 额 宝 余额宝 转 不 进 去 进去      余 转 不 出 吗      为 余 转 不 进   \n",
       "\n",
       "             word_cut_q1                word_cut_q2  \n",
       "0          要 取 消 预 约 理 财            怎 么 取 消 定 期 理 财  \n",
       "1  银 行 卡 密 码 正 确 但 是 错 误        密 码 输 入 正 确 提 示 错 误  \n",
       "2    自 动 还 款 是 扣 余 额 宝 吗  余 额 宝 有 钱 还 款 当 自 动 扣 钱 吗  \n",
       "3  赔 付 运 费 险 什 么 时 候 到 账        退 货 什 么 时 候 退 运 费 给  \n",
       "4        余 额 宝 转 不 出 来 吗        为 什 么 余 额 宝 转 不 进 去  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_1 = time.time()\n",
    "replace_dict = {'零':'0','一':'1','二':'2','三':'3','四':'4','五':'5','六':'6','七':'7','八':'8','九':'9','十':'10','叻': '了', '童': '通', '职': '值', '電': '电', '刪': '删', '宫': '营', '轲': '可', '兩': '两', '泽': '择', '拱': '供', '貝': '呗', '夠': '够', '罰': '罚', '嚒': '么', '涉': '设', '愈': '逾', '唄': '呗', '為': '为', '現': '现', '珊': '删', '酱': '降', '讷': '呐', '杞': '从', '竖': '公', '戗': '钱', '凊': '清', '陪': '赔', '嫌': '限', '鹅': '额', '聪': '充', '個': '个', '亿': '已', '蚂议': '蚂蚁', '陶': '淘', '伏': '付', '堡': '宝', '肔': '服', '巳': '已', '花坝': '花呗', '洼': '清', '甜': '填', '厉': '里', '渣': '咋', '纬': '为', '無': '无', '勇': '用', '扭': '钮', '压金': '押金', '绐': '给', '捉': '提', '喻': '逾', '換': '还', '胞': '宝', '調': '调', '抄': '超', '麽': '么', '歆': '完', '囗': '口', '卜': '不', '扮': '办', '氣': '气', '費': '费', '評': '评', '夫': '付', '猛': '能', '銀': '银', '枪': '清', '痛': '通', '喀': '额', '囙': '回', '筠': '运', '昰': '是', '吾': '我', '帳': '账', '玉': '与', '浓': '弄', '雪': '学', '螞蟻': '蚂蚁', '規': '规', '丕': '不', '時': '时', '花唄': '花呗', '梓': '么', '济': '机', '弍': '式', '貸': '贷', '繳': '交', '届': '借', '甪': '用', '蒋': '降', '欺': '期', '舍': '设', '妃': '为', '咔': '卡', '啤': '碑', '傲': '以', '俞': '逾', '田': '天', '剛': '刚', '怼': '对', '脚': '交', '怅': '账', '餮': '餐', '栓': '删', '揍': '款', '寶': '宝', '蝙': '变', '肖': '消', '剪': '减', '崔': '催', '榜': '绑', '扎': '咋', '圆': '元', '饯': '钱', '嘞': '了', '腐': '付', '辞': '迟', '昱': '里', '师': '是', '侍': '待', '睌': '晚', '宣': '删', '花背': '花呗', '購': '购', '慨': '概', '魔': '摩', '臂': '呗', '肿': '怎', '花贝': '花呗', '碼': '码', '茨': '款', '拳': '券', '乍': '咋', '証': '证', '歧': '期', '嘟': '都', '结呗': '借呗', '锤': '吹', '轻': '清', '厚': '后', '玏': '功', '乙': '已', '挷': '绑', '拦': '栏', '辟': '批', '讠': '之', '闹': '弄', '負': '付', '犹': '怀', '筘': '扣', '嗳': '爱', '說': '说', '扔': '仍', '花裁': '花', '吋': '时', '収': '收', '磕': '可', '給': '给', '腿': '退', '梆': '绑', '冬': '冻', '幼': '动', '炸': '咋', '經': '经', '骂': '吗', '欹': '款', '莉': '里', '叶': '页', '鍀': '的', '岀': '出', '欲': '逾', '花被': '花呗', '节清': '结清', '錢': '钱', '曰': '日', '戒备': '借呗', '灣': '湾', '贺': '和', '紅': '红', '幺': '么', '孒': '了', '別': '别', '涮': '刷', '歉': '欠', '泥': '呢', '額': '额', '栅': '删', '佝': '何', '壮': '状', '叹': '呗', '眷': '劵', '洋': '样', '蚂蚊': '蚂蚁', '哩': '里', '蚂蚱': '蚂蚁', '還': '还', '樣': '样', '杳': '查', '茌': '花', '卷': '券', '證': '证', '麼': '么', '佘': '余', '買': '买', '帝': '低', '胀': '账', '雨': '与', '花能': '花呗能', '崴': '为', '貨': '货', '丟': '丢', '開': '开', '叭': '呗', '昵': '呢', '祝': '况', '毙': '闭', '屎': '是', '佰': '百', '宴': '延', '幵': '开', '仟': '千', '來': '来', '挨': '爱', '祢': '你', '糸': '细', '颃': '用', '乳': '用', '借唄': '借呗', '唔': '客', '則': '则', '阔': '可', '叨': '嘛', '花多上': '花多少', '镀': '度', '刭': '到', '冯': '吗', '蔡': '才', '丽': '里', '減': '减', '狂': '款', '錯': '错', '匆': '充', '問': '问', '窃': '切', '贯': '关', '勒': '了', '颌': '额', '敗': '败', '咬': '要', '鈤': '日', '莪': '我', '腨': '用', '吵': '超', '篮': '蓝', '培': '赔', '粑': '把', '躲': '多', '嗎': '吗', '戶': '户', '毎': '每', '呮': '呗', '姑': '过', '胆': '但', '脱': '拖', '胃': '为', '剧': '刷', '吸': '息', '布': '不', '夂': '久', '栋': '冻', '淸': '清', '萌': '能', '愉': '逾', '請': '请', '卅': '啥', '堤': '提', '吱': '支', '禾': '何', '菅': '营', '渝': '逾', '侯': '候', '權': '权', '鼻': '比', '杜': '度', '嗨': '还', '踩': '才', '矿': '款', '珐': '法', 'ma': '吗', '花百': '花呗', '绊': '绑', '甬': '通', '車': '车', '叧': '另', '述': '诉', '査': '查', '瞪': '登', '機': '机', '啲': '的', '設': '设', '綁': '绑', '遲': '迟', '赞': '暂', '粤': '月', '驗': '验', '説': '说', '花花': '花', '坝': '呗', '发呗': '花呗', '虫': '了', '臨': '临', '笫': '第', '廷': '延', '琪': '期', '扥': '等', '谝': '骗', '倩': '欠', '挑': '调', '⑩': '', '/': '', '‘': '', '哩': '', '～±': '', '／': '', '｛': '', ';': '', '：': '', '％': '', '＝': '', '按': '', '喏': '', '>': '', '俺们': '', '》': '', 'Ⅲ': '', '蜂': '', '*': '', '日': '', '=': '', '⑦': '', '～': '', '）': '', ']': '', '。': '', ',': '', '“': '', '}': '', '逼': '', '＞': '', 'ＬＩ': '', '&': '', '(': '', \"'\": '', '哎哟': '', '数': '', 'sub': '', '！': '', '~': '', '@': '', '∈': '', '咦': '', '?': '', '喔唷': '', '⑤': '', '①': '', 'μ': '', '、': '', 'γ': '', '嗳': '', '』': '', '一': '', '②': '', 'Ｉ': '', 'В': '', '＃': '', '兮': '', '我': '', '［': '', '＋': '', '把': '', 'Ｌ': '', '<': '', '俺': '', '──': '', '⑧': '', '[': '', '④': '', 'sup': '', '哎呀': '', '；': '', '哎': '', 'Ｂ': '', '÷': '', '呗': '', '阿': '', '喔': '', '蚂': '', 'ａ': '', '#': '', 'Ｆ': '', '〔': '', ':': '', '吧': '', '丫': '', '嗯': '', '的': '', '■': '', '”': '', 'Ｔ': '', '＇': '', '《': '', '啐': '', '也': '', '嗬': '', '㈧': '', 'φ': '', '\"': '', '↑': '', 'Δ': '', 'Ψ': '', '℃': '', '⑥': '', '〕': '', '+': '', 'exp': '', '＆': '', '罢了': '', '·': '', '″': '', '—': '', '１': '', '〉': '', '...': '', '＊': '', 'Lex': '', '＿': '', '蚁': '', '啊': '', '『': '', '．': '', '【': '', '呃': '', '［－': '', '▲': '', '，': '', '’': '', '｝': '', '（': '', '－': '', '】': '', '×': '', '咧': '', '.': '', '了': '', ')': '', 'Ａ': '', 'Ｘ': '', '咚': '', '］': '', '？': '', '→': '', '③': '', '＜': '', '吧哒': '', '按照': '', '唉': '', '＞λ': '', '———': '', '-': '', 'Ｚ': '', '⑨': '', '～＋': '', '呐': ''}\n",
    "# all_data['question1'] = all_data['question1'].map(replace_dict)\n",
    "# all_data['question2'] = all_data['question2'].map(replace_dict)\n",
    "\n",
    "# all_data.replace({'question1': replace_dict, 'question2': replace_dict})\n",
    "\n",
    "all_data['question1'] = all_data.iloc[:, 1].apply(lambda x: replace_words(x))\n",
    "all_data['question2'] = all_data.iloc[:, 2].apply(lambda x: replace_words(x))\n",
    "print('替换词: ', time.time() - time_1)\n",
    "time_15 = time.time()\n",
    "all_data['jieba_q1'] = all_data.iloc[:, 1].apply(lambda x: jieba_cut(x))\n",
    "all_data['jieba_q2'] = all_data.iloc[:, 2].apply(lambda x: jieba_cut(x))\n",
    "print('jieba分词：', time.time() - time_15)\n",
    "time_2 = time.time()\n",
    "all_data['jieba_word_cut_q1'] = all_data.jieba_q1.apply(lambda x: jieba_word_cut(x))\n",
    "all_data['jieba_word_cut_q2'] = all_data.jieba_q2.apply(lambda x: jieba_word_cut(x))\n",
    "print('分字+词: ', time.time() - time_2)\n",
    "time_3 = time.time()\n",
    "all_data['first_char_q1'] = all_data.jieba_q1.apply(lambda x: first_char_cut(x))\n",
    "all_data['first_char_q2'] = all_data.jieba_q2.apply(lambda x: first_char_cut(x))\n",
    "print('单词首字: ', time.time() - time_3)\n",
    "time_4 = time.time()\n",
    "all_data['word_cut_q1'] = all_data.iloc[:, 1].apply(lambda x: word_cut(x))\n",
    "all_data['word_cut_q2'] = all_data.iloc[:, 2].apply(lambda x: word_cut(x))\n",
    "print('分字: ', time.time() - time_4)\n",
    "print('总时间：', time.time() - time_1)\n",
    "all_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''IF-IDF'''\n",
    "stopwords = get_stopwords()\n",
    "text_data = []\n",
    "for idx,row in all_data.iterrows():\n",
    "    text_data.append(row['cut_question1'])\n",
    "    text_data.append(row['cut_question2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(204954, 12098)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# term-count matrix\n",
    "count_vectorizer = CountVectorizer(input='content', analyzer='word', stop_words=stopwords, lowercase=False)\n",
    "count_matrix = count_vectorizer.fit_transform(text_data)\n",
    "count_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# term-tfidf matrix\n",
    "tfidf_vectorizer = TfidfVectorizer(input='content', analyzer='word',stop_words=stopwords,\n",
    "                                   lowercase=False,use_idf=True, sublinear_tf=True)\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Latent Semantic Analysis'''\n",
    "lsa = TruncatedSVD(n_components=200, n_iter=10, random_state=123)\n",
    "count_lsa_vector = lsa.fit_transform(count_matrix)\n",
    "idf_lsa_vector = lsa.fit_transform(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特征: 计算（s1的字同时在s2中也出现和s2的字同时在s1中也出现）的比例\n",
    "def shared_word_proportion(row):\n",
    "        q1words = {}\n",
    "        q2words = {}\n",
    "        for word in row['jieba_word_cut_q1'].split():\n",
    "            q1words[word] = q1words.get(word, 0) + 1\n",
    "        for word in row['jieba_word_cut_q2'].split():\n",
    "            q2words[word] = q2words.get(word, 0) + 1\n",
    "        n_shared_word_in_q1 = sum([q1words[w] for w in q1words if w in q2words])\n",
    "        n_shared_word_in_q2 = sum([q2words[w] for w in q2words if w in q1words])\n",
    "        n_tol = sum(q1words.values()) + sum(q2words.values())\n",
    "        if 1e-6 > n_tol:\n",
    "            return 0.\n",
    "        else:\n",
    "            return 1.0 * (n_shared_word_in_q1 + n_shared_word_in_q2) / n_tol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['shared_word'] = all_data.apply(shared_word_proportion, axis=1, raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算所有词的tfidf值\n",
    "def init_idf(data):\n",
    "    idf = {}\n",
    "    q_set = set()\n",
    "    for index, row in data.iterrows():\n",
    "        q1 = row['jieba_word_cut_q1']\n",
    "        q2 = row['jieba_word_cut_q2']\n",
    "        if q1 not in q_set:\n",
    "            q_set.add(q1)\n",
    "            words = q1.split()\n",
    "            for word in words:\n",
    "                idf[word] = idf.get(word, 0) + 1\n",
    "        if q2 not in q_set:\n",
    "            q_set.add(q2)\n",
    "            words = q2.split()\n",
    "            for word in words:\n",
    "                idf[word] = idf.get(word, 0) + 1\n",
    "    num_docs = len(data)\n",
    "    for word in idf:\n",
    "        idf[word] = math.log(num_docs / (idf[word] + 1.)) / math.log(2.)\n",
    "    return idf\n",
    "idf = init_idf(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特征：共现词的tfidf比例（跟上面shared_word_proportion类似）\n",
    "def tfidf_shared_word(row):\n",
    "    q1words = {}\n",
    "    q2words = {}\n",
    "    for word in row['jieba_word_cut_q1'].split():\n",
    "        q1words[word] = q1words.get(word, 0) + 1\n",
    "    for word in row['jieba_word_cut_q2'].split():\n",
    "        q2words[word] = q2words.get(word, 0) + 1\n",
    "    sum_shared_word_in_q1 = sum([q1words[w] * idf.get(w, 0) for w in q1words if w in q2words])\n",
    "    sum_shared_word_in_q2 = sum([q2words[w] * idf.get(w, 0) for w in q2words if w in q1words])\n",
    "    sum_tol = sum(q1words[w] * idf.get(w, 0) for w in q1words) + sum(q2words[w] * idf.get(w, 0) for w in q2words)\n",
    "    if 1e-6 > sum_tol:\n",
    "        return 0.\n",
    "    else:\n",
    "        return 1.0 * (sum_shared_word_in_q1 + sum_shared_word_in_q2) / sum_tol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['tfidf_shared'] = all_data.apply(tfidf_shared_word, axis=1, raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特征：两个句子的tfidf总和的差值\n",
    "def tfidf_dif(row):\n",
    "    q1words = {}\n",
    "    q2words = {}\n",
    "    for word in row['jieba_word_cut_q1'].split():\n",
    "        q1words[word] = q1words.get(word, 0) + 1\n",
    "    for word in row['jieba_word_cut_q2'].split():\n",
    "        q2words[word] = q2words.get(word, 0) + 1\n",
    "    tfidf_q1 = sum([q1words[w] * idf.get(w, 0) for w in q1words])\n",
    "    tfidf_q2 = sum([q2words[w] * idf.get(w, 0) for w in q2words])\n",
    "    return abs(tfidf_q1 - tfidf_q2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['tfidf_dif'] = all_data.apply(tfidf_dif, axis=1, raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特征：长度特征\n",
    "all_data['word_len1'] = all_data.first_char_q1.apply(lambda x: len(x.split()))\n",
    "all_data['word_len2'] = all_data.first_char_q2.apply(lambda x: len(x.split()))\n",
    "all_data['char_len1'] = all_data.word_cut_q1.apply(lambda x: len(x.split()))\n",
    "all_data['char_len2'] = all_data.word_cut_q2.apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特征：两个句子长度差\n",
    "def length_dif(row):\n",
    "    len_s1 = len(row['word_cut_q1'].split())\n",
    "    len_s2 = len(row['word_cut_q2'].split())\n",
    "    len_dif = abs(len_s1 - len_s2)\n",
    "    return len_dif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['length_dif'] = all_data.apply(length_dif, axis=1, raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特征：两个句子的长度差比例\n",
    "def length_dif_rate(row):\n",
    "    len_q1 = len(row['word_cut_q1'].split())\n",
    "    len_q2 = len(row['word_cut_q2'].split())\n",
    "    if max(len_q1, len_q2) < 1e-6:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return 1.0 * min(len_q1, len_q2) / max(len_q1, len_q2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['length_dif_rate'] = all_data.apply(length_dif_rate, axis=1, raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特征：共现词个数\n",
    "def common_chars(row):\n",
    "    s1 = set(row['word_cut_q1'].split())\n",
    "    s2 = set(row['word_cut_q2'].split())\n",
    "    intersection = s1.intersection(s2)\n",
    "    return len(intersection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['common_words'] = all_data.apply(common_chars, axis=1, raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特征：莱文斯顿距离\n",
    "def levenshtein_dist(row):\n",
    "    s1 = row['word_cut_q1'].split()\n",
    "    s2 = row['word_cut_q2'].split()\n",
    "    if len(s1) < len(s2):\n",
    "        temp = s1\n",
    "        s1 = s2\n",
    "        s2 = temp\n",
    "\n",
    "    # len(s1) >= len(s2)\n",
    "    if len(s2) == 0:\n",
    "        return len(s1)\n",
    "\n",
    "    previous_row = range(len(s2) + 1)\n",
    "    for i, c1 in enumerate(s1):\n",
    "        current_row = [i + 1]\n",
    "        for j, c2 in enumerate(s2):\n",
    "            insertions = previous_row[j + 1] + 1 # j+1 instead of j since previous_row and current_row are one character longer\n",
    "            deletions = current_row[j] + 1       # than s2\n",
    "            substitutions = previous_row[j] + (c1 != c2)\n",
    "            current_row.append(min(insertions, deletions, substitutions))\n",
    "        previous_row = current_row\n",
    "    return 1.0 * previous_row[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['levenshtein'] = all_data.apply(levenshtein_dist, axis=1, raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特征：否定词出现情况，一个有否定词，另一个没有，则是不相似的\n",
    "def neg_word(row):\n",
    "    neg_words = ['不','非','无','未','不曾','没','没有','别','勿','请勿','不用','无须','并非','毫无','决不','休想','永不','不要','未尝','未曾','毋','莫','从不','从未','从未有过','尚未','一无','并未','尚无','从来不','从没','绝非','远非','切莫','永不','休想','绝不','毫不','不必','禁止','忌','拒绝','杜绝','否','弗','木有']\n",
    "    s1 = row['jieba_word_cut_q1'].split()\n",
    "    s2 = row['jieba_word_cut_q2'].split()\n",
    "    s1_inter = set(s1).intersection(neg_words)\n",
    "    s2_inter = set(s2).intersection(neg_words)\n",
    "    if len(s1_inter)>0 and len(s2_inter)>0:\n",
    "        return 1\n",
    "    elif len(s1_inter)==0 and len(s2_inter)==0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['neg_word'] = all_data.apply(neg_word, axis=1, raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特征：数字出现情况，一个有数字，另一个没数字，则不相似\n",
    "def digit_in_sent(row):\n",
    "    p = re.compile(r'\\d+')\n",
    "    digit_s1 = p.findall(row['question1'])\n",
    "    digit_s2 = p.findall(row['question2'])\n",
    "    s1_count = len(digit_s1)\n",
    "    s2_count = len(digit_s2)\n",
    "    pair_and = int((0 < s1_count) and (0 < s2_count))\n",
    "    pair_or = int((0 < s1_count) or (0 < s2_count))\n",
    "    return [s1_count, s2_count, pair_and, pair_or]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['digit_in_sent'] = all_data.apply(digit_in_sent, axis=1, raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特征：只取出词语的第一个字，来计算相似度\n",
    "def first_char_shared(row):\n",
    "    s1 = set(row['first_char_q1'].split())\n",
    "    s2 = set(row['first_char_q2'].split())\n",
    "    shared_num = len(s1.intersection(s2))\n",
    "    return shared_num\n",
    "def first_char_jaccard(row):\n",
    "    s1 = set(row['first_char_q1'].split())\n",
    "    s2 = set(row['first_char_q2'].split())\n",
    "    intersection = s1.intersection(s2)\n",
    "    union = s1.union(s2)\n",
    "    return len(intersection)/len(union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['1st_shared'] = all_data.apply(first_char_shared, axis=1, raw=True)\n",
    "all_data['1st_jaccard'] = all_data.apply(first_char_jaccard, axis=1, raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_powerful_word(data):\n",
    "    \"\"\"\n",
    "    计算数据中词语的影响力，格式如下：\n",
    "    词语-->[0. 出现语句对数量，1. 出现语句对比例，2. 正确语句对比例，3. 单侧语句对比例，4. 单侧语句对正确比例，5. 双侧语句对比例，6. 双侧语句对正确比例]\n",
    "    \"\"\"\n",
    "    words_power = {}\n",
    "    for index, row in data.iterrows():\n",
    "        label = int(row['is_duplicate'])\n",
    "        q1_words = row['jieba_word_cut_q1'].split()\n",
    "        q2_words = row['jieba_word_cut_q2'].split()\n",
    "        all_words = set(q1_words + q2_words)\n",
    "        q1_words = set(q1_words)\n",
    "        q2_words = set(q2_words)\n",
    "        for word in all_words:\n",
    "            if word not in words_power:\n",
    "                words_power[word] = [0. for i in range(7)]\n",
    "            # 计算出现语句对数量\n",
    "            words_power[word][0] += 1.\n",
    "            words_power[word][1] += 1.\n",
    "            if ((word in q1_words) and (word not in q2_words)) or ((word not in q1_words) and (word in q2_words)):\n",
    "                # 计算单侧语句数量\n",
    "                words_power[word][3] += 1.\n",
    "                if 0 == label:\n",
    "                    # 计算正确语句对数量\n",
    "                    words_power[word][2] += 1.\n",
    "                    # 计算单侧语句正确比例\n",
    "                    words_power[word][4] += 1.\n",
    "            if (word in q1_words) and (word in q2_words):\n",
    "                # 计算双侧语句数量\n",
    "                words_power[word][5] += 1.\n",
    "                if 1 == label:\n",
    "                    # 计算正确语句对数量\n",
    "                    words_power[word][2] += 1.\n",
    "                    # 计算双侧语句正确比例\n",
    "                    words_power[word][6] += 1.\n",
    "    for word in words_power:\n",
    "        # 计算出现语句对比例\n",
    "        words_power[word][1] /= len(data)\n",
    "        # 计算正确语句对比例\n",
    "        words_power[word][2] /= words_power[word][0]\n",
    "        # 计算单侧语句对正确比例\n",
    "        if words_power[word][3] > 1e-6:\n",
    "            words_power[word][4] /= words_power[word][3]\n",
    "        # 计算单侧语句对比例\n",
    "        words_power[word][3] /= words_power[word][0]\n",
    "        # 计算双侧语句对正确比例\n",
    "        if words_power[word][5] > 1e-6:\n",
    "            words_power[word][6] /= words_power[word][5]\n",
    "        # 计算双侧语句对比例\n",
    "        words_power[word][5] /= words_power[word][0]\n",
    "    sorted_words_power = sorted(words_power.items(), key=lambda d: d[1][0], reverse=True)\n",
    "    return sorted_words_power\n",
    "\n",
    "power_words = generate_powerful_word(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38535"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(power_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_powerful_word_dside(pword, thresh_num, thresh_rate):\n",
    "    pword_dside = []\n",
    "    pword = filter(lambda x: x[1][0] * x[1][5] >= thresh_num, pword)\n",
    "    pword_sort = sorted(pword, key=lambda d: d[1][6], reverse=True)\n",
    "    pword_dside.extend(map(lambda x: x[0], filter(lambda x: x[1][6] >= thresh_rate, pword_sort)))\n",
    "    return pword_dside\n",
    "power_word_dside = init_powerful_word_dside(power_words, 20000, 0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(power_word_dside)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def power_dside(row):\n",
    "    tags = []\n",
    "    q1_words = row['jieba_word_cut_q1'].split()\n",
    "    q2_words = row['jieba_word_cut_q2'].split()\n",
    "    for word in power_word_dside:\n",
    "        if (word in q1_words) and (word in q2_words):\n",
    "            tags.append(1.0)\n",
    "        else:\n",
    "            tags.append(0.0)\n",
    "    return tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['power_dside'] = all_data.apply(power_dside, axis=1, raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_powerful_word_oside(pword, thresh_num, thresh_rate):\n",
    "    pword_oside = []\n",
    "    pword = filter(lambda x: x[1][0] * x[1][3] >= thresh_num, pword)\n",
    "    pword_oside.extend(map(lambda x: x[0], filter(lambda x: x[1][4] >= thresh_rate, pword)))\n",
    "    return pword_oside\n",
    "power_word_oside = init_powerful_word_oside(power_words, 30000, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(power_word_oside)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def power_oside(row):\n",
    "    tags = []\n",
    "    q1_words = set(row['jieba_word_cut_q1'].split())\n",
    "    q2_words = set(row['jieba_word_cut_q2'].split())\n",
    "    for word in power_word_oside:\n",
    "        if (word in q1_words) and (word not in q2_words):\n",
    "            tags.append(1.0)\n",
    "        elif (word not in q1_words) and (word in q2_words):\n",
    "            tags.append(1.0)\n",
    "        else:\n",
    "            tags.append(0.0)\n",
    "    return tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['power_oside'] = all_data.apply(power_oside, axis=1, raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pword_dside_rate(row):\n",
    "    num_least = 300\n",
    "    rate = [1.0]\n",
    "    q1_words = set(row['jieba_word_cut_q1'].split())\n",
    "    q2_words = set(row['jieba_word_cut_q2'].split())\n",
    "    share_words = list(q1_words.intersection(q2_words))\n",
    "    for word in share_words:\n",
    "        if word not in power_words:\n",
    "            continue\n",
    "        if power_words[word][0] * power_words[word][5] < num_least:\n",
    "            continue\n",
    "        rate[0] *= (1.0 - power_words[word][6])\n",
    "    rate = [1 - num for num in rate]\n",
    "    return rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_data['pword_dside_rate'] = all_data.apply(pword_dside_rate, axis=1, raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pword_oside_rate(row):\n",
    "    num_least = 300\n",
    "    rate = [1.0]\n",
    "    q1_words = set(row['jieba_word_cut_q1'].split())\n",
    "    q2_words = set(row['jieba_word_cut_q2'].split())\n",
    "    q1_diff = list(q1_words.difference(q2_words))\n",
    "    q2_diff = list(q2_words.difference(q1_words))\n",
    "    all_diff = set(q1_diff + q2_diff)\n",
    "    for word in all_diff:\n",
    "        if word not in power_words:\n",
    "            continue\n",
    "        if power_words[word][0] * power_words[word][3] < num_least:\n",
    "            continue\n",
    "        rate[0] *= (1.0 - power_words[word][4])\n",
    "    rate = [1 - num for num in rate]\n",
    "    return rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_data['pword_oside_rate'] = all_data.apply(pword_oside_rate, axis=1, raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特征：TFIDF\n",
    "def init_tfidf():\n",
    "    tfidf = TfidfVectorizer(input='content', analyzer='word',lowercase=False,use_idf=True, sublinear_tf=True)\n",
    "    tfidf_txt = pd.Series(all_data['jieba_word_cut_q1'].tolist() + all_data['jieba_word_cut_q2'].tolist()).astype(str)\n",
    "    tfidf.fit_transform(tfidf_txt)\n",
    "    return tfidf\n",
    "tfidf = init_tfidf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_fs(row):\n",
    "    q1 = row['jieba_word_cut_q1']\n",
    "    q2 = row['jieba_word_cut_q2']\n",
    "    fs = list()\n",
    "    fs.append(np.sum(tfidf.transform([str(q1)]).data))\n",
    "    fs.append(np.sum(tfidf.transform([str(q2)]).data))\n",
    "    fs.append(np.mean(tfidf.transform([str(q1)]).data))\n",
    "    fs.append(np.mean(tfidf.transform([str(q2)]).data))\n",
    "    fs.append(len(tfidf.transform([str(q1)]).data))\n",
    "    fs.append(len(tfidf.transform([str(q2)]).data))\n",
    "    return fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_data['tfidf_fs'] = all_data.apply(tfidf_fs, axis=1, raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特征：句子在语料中的重复次数\n",
    "def duplicate_num():\n",
    "    dup_num = {}\n",
    "    for index, row in all_data.iterrows():\n",
    "        q1 = row['question1']\n",
    "        q2 = row['question2']\n",
    "        dup_num[q1] = dup_num.get(q1, 0) + 1\n",
    "        if q1 != q2:\n",
    "            dup_num[q2] = dup_num.get(q2, 0) + 1\n",
    "    return dup_num\n",
    "dup_num = duplicate_num()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def duplicate_sent(row):\n",
    "    s1 = row['question1']\n",
    "    s2 = row['question2']\n",
    "    s1_num = dup_num[s1]\n",
    "    s2_num = dup_num[s2]\n",
    "    return [s1_num, s2_num, max(s1_num, s2_num), min(s1_num, s2_num)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['duplicate_sent'] = all_data.apply(duplicate_sent, axis=1, raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 距离计算函数！\n",
    "def jaccard_coef(A, B):\n",
    "    if not isinstance(A, set):\n",
    "        A = set(A)\n",
    "    if not isinstance(B, set):\n",
    "        B = set(B)\n",
    "    return float(len(A.intersection(B)) / len(A.union(B)))\n",
    "\n",
    "def dice_dist(A, B):\n",
    "    if not isinstance(A, set):\n",
    "        A = set(A)\n",
    "    if not isinstance(B, set):\n",
    "        B = set(B)\n",
    "    return (2.0 * float(len(A.intersection(B)))) / (len(A) + len(B))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算 N-gram.\n",
    "def unigrams(words):\n",
    "    assert type(words) == list\n",
    "    return words\n",
    "\n",
    "def bigrams(words, join_string, skip=0):\n",
    "    assert type(words) == list\n",
    "    L = len(words)\n",
    "    if L > 1:\n",
    "        lst = []\n",
    "        for i in range(L - 1):\n",
    "            for k in range(1, skip + 2):\n",
    "                if i + k < L:\n",
    "                    lst.append(join_string.join([words[i], words[i + k]]))\n",
    "    else:\n",
    "        # set it as unigram\n",
    "        lst = unigrams(words)\n",
    "    return lst\n",
    "\n",
    "def trigrams(words, join_string, skip=0):\n",
    "    assert type(words) == list\n",
    "    L = len(words)\n",
    "    if L > 2:\n",
    "        lst = []\n",
    "        for i in range(L - 2):\n",
    "            for k1 in range(1, skip + 2):\n",
    "                for k2 in range(1, skip + 2):\n",
    "                    if i + k1 < L and i + k1 + k2 < L:\n",
    "                        lst.append(join_string.join([words[i], words[i + k1], words[i + k1 + k2]]))\n",
    "    else:\n",
    "        # set it as bigram\n",
    "        lst = bigrams(words, join_string, skip)\n",
    "    return lst\n",
    "\n",
    "def fourgrams(words, join_string):\n",
    "    assert type(words) == list\n",
    "    L = len(words)\n",
    "    if L > 3:\n",
    "        lst = []\n",
    "        for i in xrange(L - 3):\n",
    "            lst.append(join_string.join([words[i], words[i + 1], words[i + 2], words[i + 3]]))\n",
    "    else:\n",
    "        # set it as trigram\n",
    "        lst = trigrams(words, join_string)\n",
    "    return lst\n",
    "\n",
    "def ngrams(words, ngram, join_string=\" \"):\n",
    "    if ngram == 1:\n",
    "        return unigrams(words)\n",
    "    elif ngram == 2:\n",
    "        return bigrams(words, join_string)\n",
    "    elif ngram == 3:\n",
    "        return trigrams(words, join_string)\n",
    "    elif ngram == 4:\n",
    "        return fourgrams(words, join_string)\n",
    "    elif ngram == 12:\n",
    "        unigram = unigrams(words)\n",
    "        bigram = [x for x in bigrams(words, join_string) if len(x.split(join_string)) == 2]\n",
    "        return unigram + bigram\n",
    "    elif ngram == 123:\n",
    "        unigram = unigrams(words)\n",
    "        bigram = [x for x in bigrams(words, join_string) if len(x.split(join_string)) == 2]\n",
    "        trigram = [x for x in trigrams(words, join_string) if len(x.split(join_string)) == 3]\n",
    "        return unigram + bigram + trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特征：n-gram jaccard系数\n",
    "def ngram_jaccard(row):\n",
    "    q1_words = row['jieba_word_cut_q1'].split()\n",
    "    q2_words = row['jieba_word_cut_q2'].split()\n",
    "    fs = list()\n",
    "    for n in range(1, 4):\n",
    "        q1_ngrams = ngrams(q1_words, n)\n",
    "        q2_ngrams = ngrams(q2_words, n)\n",
    "        fs.append(jaccard_coef(q1_ngrams, q2_ngrams))\n",
    "    return fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['ngram_jaccard'] = all_data.apply(ngram_jaccard, axis=1, raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特征：n-gram dice系数\n",
    "def ngram_dice(row):\n",
    "    q1_words = row['jieba_word_cut_q1'].split()\n",
    "    q2_words = row['jieba_word_cut_q2'].split()\n",
    "    fs = list()\n",
    "    for n in range(1, 4):\n",
    "        q1_ngrams = ngrams(q1_words, n)\n",
    "        q2_ngrams = ngrams(q2_words, n)\n",
    "        fs.append(dice_dist(q1_ngrams, q2_ngrams))\n",
    "    return fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['ngram_dice'] = all_data.apply(ngram_dice, axis=1, raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算所有字的tfidf值\n",
    "def init_idf(data):\n",
    "    idf = {}\n",
    "    q_set = set()\n",
    "    for index, row in data.iterrows():\n",
    "        q1 = row['word_cut_q1']\n",
    "        q2 = row['word_cut_q2']\n",
    "        if q1 not in q_set:\n",
    "            q_set.add(q1)\n",
    "            words = q1.split()\n",
    "            for word in words:\n",
    "                idf[word] = idf.get(word, 0) + 1\n",
    "        if q2 not in q_set:\n",
    "            q_set.add(q2)\n",
    "            words = q2.split()\n",
    "            for word in words:\n",
    "                idf[word] = idf.get(word, 0) + 1\n",
    "    num_docs = len(data)\n",
    "    for word in idf:\n",
    "        idf[word] = math.log(num_docs / (idf[word] + 1.)) / math.log(2.)\n",
    "    return idf\n",
    "idf = init_idf(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "635788"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preTrainWordEmbedding():\n",
    "    # 读取预训练的词向量，返回Embedding Dict\n",
    "    embedding_dict = {}\n",
    "    f = codecs.open('./word_embedding/sgns.baidubaike.bigram-char', 'r', encoding='utf-8')\n",
    "    for line in f:\n",
    "        line = line.split()\n",
    "        if len(line) > 0:\n",
    "            word = line[0]\n",
    "            if len(line) == 301:\n",
    "                embed_vec = np.array(line[1:], dtype=\"float32\")\n",
    "                embedding_dict[word] = embed_vec\n",
    "    f.close()\n",
    "    return embedding_dict\n",
    "\n",
    "embedding_dict = preTrainWordEmbedding()\n",
    "len(embedding_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2875"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 计算词典\n",
    "word_dict = {}\n",
    "for idx,row in all_data.iterrows():\n",
    "    text = row['word_cut_q1'].split() + row['word_cut_q2'].split()\n",
    "    for word in text:\n",
    "        if word not in word_dict:\n",
    "            word_dict[word] = len(word_dict)\n",
    "len(word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_count = 0\n",
    "w2v_dict = {}\n",
    "for word in word_dict:\n",
    "    if word not in w2v_dict:\n",
    "        if word in embedding_dict:\n",
    "            w2v_dict[word] = embedding_dict.get(word)\n",
    "            load_count += 1\n",
    "        else:\n",
    "            w2v_dict[word] = np.random.randn(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2776"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特征：句子中每个单词的词向量之和即为句子向量，两个句向量之间的cosine similarity\n",
    "def w2v_cos(row):\n",
    "    q1_words = row['word_cut_q1'].split()\n",
    "    q2_words = row['word_cut_q2'].split()\n",
    "    q1_vec = np.array(300 * [0.])\n",
    "    q2_vec = np.array(300 * [0.])\n",
    "\n",
    "    for word in q1_words:\n",
    "        if word in w2v_dict:\n",
    "            q1_vec = q1_vec + w2v_dict[word]\n",
    "    for word in q2_words:\n",
    "        if word in w2v_dict:\n",
    "            q2_vec = q2_vec + w2v_dict[word]\n",
    "\n",
    "    cos_sim = 0.\n",
    "    q1_vec = np.mat(q1_vec)\n",
    "    q2_vec = np.mat(q2_vec)\n",
    "    factor = np.linalg.norm(q1_vec) * np.linalg.norm(q2_vec)\n",
    "    if 1e-6 < factor:\n",
    "        cos_sim = float(q1_vec * q2_vec.T) / factor\n",
    "    return cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_vec(row):\n",
    "    q1_words = row['word_cut_q1'].split()\n",
    "    q2_words = row['word_cut_q2'].split()\n",
    "    q1_vec = np.array(300 * [0.])\n",
    "    q2_vec = np.array(300 * [0.])\n",
    "    for word in q1_words:\n",
    "        if word in w2v_dict:\n",
    "            q1_vec += w2v_dict[word]\n",
    "    for word in q2_words:\n",
    "        if word in w2v_dict:\n",
    "            q2_vec += w2v_dict[word]\n",
    "    return list(q1_vec) + list(q1_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 该特征会使内存爆炸！！！\n",
    "# all_data['sent_vec'] = all_data.apply(sent_vec, axis=1, raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特征：句子中每个单词的词向量的tfidf加权的和即为句子向量，两个句向量之间的cosine similarity\n",
    "def w2v_idf_cos(row):\n",
    "    q1_words = row['word_cut_q1'].split()\n",
    "    q2_words = row['word_cut_q2'].split()\n",
    "    q1_vec = np.array(300 * [0.])\n",
    "    q2_vec = np.array(300 * [0.])\n",
    "    \n",
    "    q1_words_cnt = {}\n",
    "    q2_words_cnt = {}\n",
    "    \n",
    "    for word in q1_words:\n",
    "        q1_words_cnt[word] = q1_words_cnt.get(word, 0.) + 1.\n",
    "    for word in q2_words:\n",
    "        q2_words_cnt[word] = q2_words_cnt.get(word, 0.) + 1.\n",
    "\n",
    "    for word in q1_words_cnt:\n",
    "        if word in w2v_dict:\n",
    "            q1_vec += idf.get(word, 0.) * q1_words_cnt[word] * w2v_dict[word]\n",
    "    for word in q2_words_cnt:\n",
    "        if word in w2v_dict:\n",
    "            q2_vec += idf.get(word, 0.) * q2_words_cnt[word] * w2v_dict[word]\n",
    "\n",
    "    cos_sim = 0.\n",
    "    q1_vec = np.mat(q1_vec)\n",
    "    q2_vec = np.mat(q2_vec)\n",
    "    factor = np.linalg.norm(q1_vec) * np.linalg.norm(q2_vec)\n",
    "    if 1e-6 < factor:\n",
    "        cos_sim = float(q1_vec * q2_vec.T) / factor\n",
    "    return cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_w2v_sim(row):\n",
    "    q1_words = row['word_cut_q1'].split()\n",
    "    q2_words = row['word_cut_q2'].split()\n",
    "    q1_idf = 0.0\n",
    "    q2_idf = 0.0\n",
    "    q1_sim_idf_sum = 0.0\n",
    "    q2_sim_idf_sum = 0.0\n",
    "    for word1 in q1_words:\n",
    "        q1_idf += idf.get(word1, 0)\n",
    "        q1_max_sim = 0.0\n",
    "        for word2 in q2_words:\n",
    "            cos_sim = float(cosine_similarity(np.mat(w2v_dict.get(word1,0)), np.mat(w2v_dict.get(word2,0))))\n",
    "            if cos_sim > q1_max_sim:\n",
    "                q1_max_sim = cos_sim\n",
    "        q1_sim_idf_sum += q1_max_sim * idf.get(word1,0)\n",
    "    for word1 in q2_words:\n",
    "        q2_idf += idf.get(word1, 0)\n",
    "        q2_max_sim = 0.0\n",
    "        for word2 in q1_words:\n",
    "            cos_sim = float(cosine_similarity(np.mat(w2v_dict.get(word1,0)), np.mat(w2v_dict.get(word2,0))))\n",
    "            if cos_sim > q2_max_sim:\n",
    "                q2_max_sim = cos_sim\n",
    "        q2_sim_idf_sum += q2_max_sim * idf.get(word1,0)\n",
    "    sim = ((q1_sim_idf_sum / q1_idf) + (q2_sim_idf_sum / q2_idf)) / 2.0\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['w2v_cos'] = all_data.apply(w2v_cos, axis=1, raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['w2v_idf_cos'] = all_data.apply(w2v_idf_cos, axis=1, raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start = time.time()\n",
    "# all_data['text_w2v_sim'] = all_data.apply(text_w2v_sim, axis=1, raw=True)\n",
    "# print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_orig = pd.read_csv('./data/all_data', sep=' ', header=None, encoding='utf-8')\n",
    "train_orig.columns = ['id', 'question1', 'question2', 'is_duplicate']\n",
    "\n",
    "df1 = train_orig[['question1']].copy()\n",
    "df2 = train_orig[['question2']].copy()\n",
    "df2.rename(columns = {'question2':'question1'},inplace=True)\n",
    "\n",
    "train_questions = df1.append(df2)\n",
    "train_questions.drop_duplicates(subset = ['question1'],inplace=True)\n",
    "\n",
    "train_questions.reset_index(inplace=True,drop=True)\n",
    "questions_dict = pd.Series(train_questions.index.values,index=train_questions.question1.values).to_dict()\n",
    "train_cp = train_orig.copy()\n",
    "\n",
    "# test_cp['is_duplicate'] = -1\n",
    "comb = train_cp\n",
    "\n",
    "comb['q1_hash'] = comb['question1'].map(questions_dict)\n",
    "comb['q2_hash'] = comb['question2'].map(questions_dict)\n",
    "\n",
    "q1_vc = comb.q1_hash.value_counts().to_dict()\n",
    "q2_vc = comb.q2_hash.value_counts().to_dict()\n",
    "\n",
    "def try_apply_dict(x,dict_to_apply):\n",
    "    try:\n",
    "        return dict_to_apply[x]\n",
    "    except KeyError:\n",
    "        return 0\n",
    "#map to frequency space\n",
    "comb['q1_freq'] = comb['q1_hash'].map(lambda x: try_apply_dict(x,q1_vc) + try_apply_dict(x,q2_vc))\n",
    "comb['q2_freq'] = comb['q2_hash'].map(lambda x: try_apply_dict(x,q1_vc) + try_apply_dict(x,q2_vc))\n",
    "\n",
    "train_comb = comb[comb['is_duplicate'] >= 0][['id','q1_hash','q2_hash','q1_freq','q2_freq','is_duplicate']]\n",
    "test_comb = comb[comb['is_duplicate'] < 0][['id','q1_hash','q2_hash','q1_freq','q2_freq']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>q1_hash</th>\n",
       "      <th>q2_hash</th>\n",
       "      <th>q1_freq</th>\n",
       "      <th>q2_freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>470276</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>470277</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>470278</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>470279</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>470280</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   q1_hash  q2_hash  q1_freq  q2_freq\n",
       "0        0   470276        1        1\n",
       "1        1   470277        1        1\n",
       "2        2   470278        1        1\n",
       "3        3   470279        2        1\n",
       "4        4   470280        1        1"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_comb.drop(columns=['id', 'is_duplicate'], inplace=True)\n",
    "train_comb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_comb.to_csv('./data/freq_feature.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id',\n",
       " 'question1',\n",
       " 'question2',\n",
       " 'is_duplicate',\n",
       " 'jieba_q1',\n",
       " 'jieba_q2',\n",
       " 'jieba_word_cut_q1',\n",
       " 'jieba_word_cut_q2',\n",
       " 'first_char_q1',\n",
       " 'first_char_q2',\n",
       " 'word_cut_q1',\n",
       " 'word_cut_q2',\n",
       " 'w2v_cos',\n",
       " 'w2v_idf_cos']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.head()\n",
    "all_data.columns.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['w2v_cos', 'w2v_idf_cos']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgb_label = all_data['is_duplicate']\n",
    "lgb_data = all_data.drop(columns=['id', 'question1', 'question2', 'is_duplicate', 'jieba_q1', 'jieba_q2', 'jieba_word_cut_q1', 'jieba_word_cut_q2', 'first_char_q1', \n",
    "                                  'first_char_q2', 'word_cut_q1', 'word_cut_q2'])\n",
    "# lgb_data = lgb_data.drop(columns=['f12_neg_word', 'f13_digit_in_sent','f3_word_len1','f4_word_len2','f5_char_len1','f6_char_len2','f7_length_dif'])\n",
    "lgb_data.head()\n",
    "lgb_data.columns.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_data['digit_in_sent'] = lgb_data.digit_in_sent.apply(lambda x: ','.join(map(str, x)))\n",
    "digit_sent = lgb_data['digit_in_sent'].str.split(',', expand=True).rename(columns = lambda x: 'digit_sent_' + str(x+1))\n",
    "digit_sent = digit_sent.apply(pd.to_numeric)\n",
    "lgb_data = pd.concat([lgb_data, digit_sent], axis=1)\n",
    "lgb_data.drop(['digit_in_sent'], axis=1, inplace=True)\n",
    "\n",
    "lgb_data['power_dside'] = lgb_data.power_dside.apply(lambda x: ','.join(map(str, x)))\n",
    "power_d = lgb_data['power_dside'].str.split(',', expand=True).rename(columns = lambda x: 'power_d_' + str(x+1))\n",
    "power_d = power_d.apply(pd.to_numeric)\n",
    "lgb_data = pd.concat([lgb_data, power_d], axis=1)\n",
    "lgb_data.drop(['power_dside'], axis=1, inplace=True)\n",
    "\n",
    "lgb_data['power_oside'] = lgb_data.power_oside.apply(lambda x: ','.join(map(str, x)))\n",
    "power_o = lgb_data['power_oside'].str.split(',', expand=True).rename(columns = lambda x: 'power_o_' + str(x+1))\n",
    "power_o = power_o.apply(pd.to_numeric)\n",
    "lgb_data = pd.concat([lgb_data, power_o], axis=1)\n",
    "lgb_data.drop(['power_oside'], axis=1, inplace=True)\n",
    "\n",
    "lgb_data['duplicate_sent'] = lgb_data.duplicate_sent.apply(lambda x: ','.join(map(str, x)))\n",
    "dup_sent = lgb_data['duplicate_sent'].str.split(',', expand=True).rename(columns = lambda x: 'dup_sent_' + str(x+1))\n",
    "dup_sent = dup_sent.apply(pd.to_numeric)\n",
    "lgb_data = pd.concat([lgb_data, dup_sent], axis=1)\n",
    "lgb_data.drop(['duplicate_sent'], axis=1, inplace=True)\n",
    "\n",
    "lgb_data['ngram_jaccard'] = lgb_data.ngram_jaccard.apply(lambda x: ','.join(map(str, x)))\n",
    "ngram_jac = lgb_data['ngram_jaccard'].str.split(',', expand=True).rename(columns = lambda x: 'ngram_jac_' + str(x+1))\n",
    "ngram_jac = ngram_jac.apply(pd.to_numeric)\n",
    "lgb_data = pd.concat([lgb_data, ngram_jac], axis=1)\n",
    "lgb_data.drop(['ngram_jaccard'], axis=1, inplace=True)\n",
    "\n",
    "lgb_data['ngram_dice'] = lgb_data.ngram_dice.apply(lambda x: ','.join(map(str, x)))\n",
    "ngram_di = lgb_data['ngram_dice'].str.split(',', expand=True).rename(columns = lambda x: 'ngram_di_' + str(x+1))\n",
    "ngram_di = ngram_di.apply(pd.to_numeric)\n",
    "lgb_data = pd.concat([lgb_data, ngram_di], axis=1)\n",
    "lgb_data.drop(['ngram_dice'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_data.columns.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns Normalization\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "lgb_data['word_len1'] = pd.DataFrame(scaler.fit_transform(lgb_data['word_len1'].values.astype(float).reshape(-1,1)))\n",
    "lgb_data['word_len2'] = pd.DataFrame(scaler.fit_transform(lgb_data['word_len2'].values.astype(float).reshape(-1,1)))\n",
    "lgb_data['char_len1'] = pd.DataFrame(scaler.fit_transform(lgb_data['char_len1'].values.astype(float).reshape(-1,1)))\n",
    "lgb_data['char_len2'] = pd.DataFrame(scaler.fit_transform(lgb_data['char_len2'].values.astype(float).reshape(-1,1)))\n",
    "lgb_data['length_dif'] = pd.DataFrame(scaler.fit_transform(lgb_data['length_dif'].values.astype(float).reshape(-1,1)))\n",
    "lgb_data['common_words'] = pd.DataFrame(scaler.fit_transform(lgb_data['common_words'].values.astype(float).reshape(-1,1)))\n",
    "lgb_data['levenshtein'] = pd.DataFrame(scaler.fit_transform(lgb_data['levenshtein'].values.astype(float).reshape(-1,1)))\n",
    "lgb_data['1st_shared'] = pd.DataFrame(scaler.fit_transform(lgb_data['1st_shared'].values.astype(float).reshape(-1,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>jieba_q1</th>\n",
       "      <th>jieba_q2</th>\n",
       "      <th>shared_word</th>\n",
       "      <th>tfidf_shared</th>\n",
       "      <th>tfidf_dif</th>\n",
       "      <th>word_len1</th>\n",
       "      <th>word_len2</th>\n",
       "      <th>char_len1</th>\n",
       "      <th>char_len2</th>\n",
       "      <th>length_dif</th>\n",
       "      <th>...</th>\n",
       "      <th>dup_sent_1</th>\n",
       "      <th>dup_sent_2</th>\n",
       "      <th>dup_sent_3</th>\n",
       "      <th>dup_sent_4</th>\n",
       "      <th>ngram_jac_1</th>\n",
       "      <th>ngram_jac_2</th>\n",
       "      <th>ngram_jac_3</th>\n",
       "      <th>ngram_di_1</th>\n",
       "      <th>ngram_di_2</th>\n",
       "      <th>ngram_di_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[要, 取消, 预约, 理财]</td>\n",
       "      <td>[怎么, 取消, 定期, 理财]</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.577543</td>\n",
       "      <td>11.628187</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.222222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[银行卡, 密码, 正确, 但是, 错误]</td>\n",
       "      <td>[密码, 输入, 正确, 提示, 错误]</td>\n",
       "      <td>0.580645</td>\n",
       "      <td>0.669128</td>\n",
       "      <td>7.723174</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.409091</td>\n",
       "      <td>0.260870</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.580645</td>\n",
       "      <td>0.413793</td>\n",
       "      <td>0.222222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[自动, 还款, 是, 扣, 余额宝, 吗]</td>\n",
       "      <td>[余额宝, 有钱, 还款, 当, 自动, 扣钱, 吗]</td>\n",
       "      <td>0.774194</td>\n",
       "      <td>0.669738</td>\n",
       "      <td>24.605320</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.482759</td>\n",
       "      <td>0.296296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[赔付, 运费, 险, 什么, 时候, 到, 账]</td>\n",
       "      <td>[退货, 什么, 时候, 退, 运费, 给]</td>\n",
       "      <td>0.620690</td>\n",
       "      <td>0.539562</td>\n",
       "      <td>0.362458</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.518519</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[余额宝, 转, 不, 出来, 吗]</td>\n",
       "      <td>[为什么, 余额宝, 转, 不, 进去]</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.307491</td>\n",
       "      <td>8.165206</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.476190</td>\n",
       "      <td>0.421053</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 85 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    jieba_q1                     jieba_q2  shared_word  \\\n",
       "0            [要, 取消, 预约, 理财]             [怎么, 取消, 定期, 理财]     0.545455   \n",
       "1      [银行卡, 密码, 正确, 但是, 错误]         [密码, 输入, 正确, 提示, 错误]     0.580645   \n",
       "2     [自动, 还款, 是, 扣, 余额宝, 吗]  [余额宝, 有钱, 还款, 当, 自动, 扣钱, 吗]     0.774194   \n",
       "3  [赔付, 运费, 险, 什么, 时候, 到, 账]       [退货, 什么, 时候, 退, 运费, 给]     0.620690   \n",
       "4         [余额宝, 转, 不, 出来, 吗]         [为什么, 余额宝, 转, 不, 进去]     0.521739   \n",
       "\n",
       "   tfidf_shared  tfidf_dif  word_len1  word_len2  char_len1  char_len2  \\\n",
       "0      0.577543  11.628187          4          4          7          8   \n",
       "1      0.669128   7.723174          5          5         11         10   \n",
       "2      0.669738  24.605320          6          7         10         13   \n",
       "3      0.539562   0.362458          7          6         11         10   \n",
       "4      0.307491   8.165206          5          5          8         10   \n",
       "\n",
       "   length_dif     ...      dup_sent_1  dup_sent_2  dup_sent_3  dup_sent_4  \\\n",
       "0           1     ...               1           9           9           1   \n",
       "1           1     ...               1           1           1           1   \n",
       "2           3     ...               1           1           1           1   \n",
       "3           1     ...               4           1           4           1   \n",
       "4           2     ...               1           8           8           1   \n",
       "\n",
       "   ngram_jac_1  ngram_jac_2  ngram_jac_3  ngram_di_1  ngram_di_2  ngram_di_3  \n",
       "0     0.375000     0.250000     0.125000    0.545455    0.400000    0.222222  \n",
       "1     0.409091     0.260870     0.125000    0.580645    0.413793    0.222222  \n",
       "2     0.666667     0.318182     0.173913    0.800000    0.482759    0.296296  \n",
       "3     0.473684     0.350000     0.250000    0.642857    0.518519    0.400000  \n",
       "4     0.352941     0.312500     0.266667    0.521739    0.476190    0.421053  \n",
       "\n",
       "[5 rows x 85 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgb_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = lgb_data.values\n",
    "y = lgb_label.values\n",
    "x, x_test, y, y_test = train_test_split(x, y, test_size=0.15, random_state=123, stratify=y, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' LightGBM '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F1值度量方法\n",
    "def threshold(i):\n",
    "    if i > 0.40:\n",
    "        return 1.0\n",
    "    else:\n",
    "        return 0.0\n",
    "def f1_metric(y_pred, train_data):\n",
    "    y_true = train_data.get_label()\n",
    "    #y_pred = np.round(y_pred)\n",
    "    y_pred = list(map(threshold, y_pred))\n",
    "    return 'f1_score', f1_score(y_true, y_pred), True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = lgb_data.columns.values.tolist()\n",
    "lgb_train = lgb.Dataset(x, label=y, feature_name=categorical_features, categorical_feature=categorical_features, free_raw_data=False)\n",
    "lgb_test = lgb.Dataset(x_test, label=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'application': 'binary', \n",
    "              'objective': 'binary',\n",
    "              'is_unbalance': 'true',\n",
    "              'boosting': 'gbdt',\n",
    "              'num_leaves': 31,\n",
    "              'feature_fraction': 0.5,\n",
    "              'bagging_fraction': 0.5,\n",
    "              'bagging_freq': 20,\n",
    "              'learning_rate': 0.05,\n",
    "              'verbose': 0\n",
    "             }\n",
    "parameters['metric'] = ['binary_logloss']\n",
    "# parameters['metric'] = ['None']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lgb.train(params=parameters, \n",
    "                  train_set=lgb_train, \n",
    "                  valid_sets=lgb_test, \n",
    "                  num_boost_round=5000, \n",
    "                  early_stopping_rounds=100,\n",
    "                  feval=f1_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' XGBoost '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F1值度量方法\n",
    "def threshold(i):\n",
    "    if i > 0.20:\n",
    "        return 1.0\n",
    "    else:\n",
    "        return 0.0\n",
    "def f1_metric(y_pred, train_data):\n",
    "    y_true = train_data.get_label()\n",
    "    #y_pred = np.round(y_pred)\n",
    "    y_pred = list(map(threshold, y_pred))\n",
    "    return 'F1', f1_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_train = xgb.DMatrix(data=x, label=y)\n",
    "xgb_test = xgb.DMatrix(data=x_test, label=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "            'booster':'gbtree',\n",
    "            'objective':'binary:logistic',\n",
    "            'eta':0.2,\n",
    "            'max_depth':10,\n",
    "            'subsample':1.0,\n",
    "            'min_child_weight':2,\n",
    "            'colsample_bytree':0.8,\n",
    "            'scale_pos_weight':0.5,\n",
    "            'eval_metric':'logloss',\n",
    "            'gamma':0.2,            \n",
    "            'lambda':0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "watchlist = [(xgb_train,'train'),(xgb_test,'val')]\n",
    "xgb_model = xgb.train(params=parameters,\n",
    "                      dtrain=xgb_train,\n",
    "                      num_boost_round=5000,\n",
    "                      evals=watchlist,\n",
    "                      early_stopping_rounds=100,\n",
    "                      feval=f1_metric,\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
